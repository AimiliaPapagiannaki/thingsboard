{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import sys\n",
    "import warnings\n",
    "pd.set_option('display.max_rows', None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_info(device, address):\n",
    "    \n",
    "    r = requests.post(address + \"/api/auth/login\",\n",
    "                      json={'username': 'meazonpro@meazon.com', 'password': 'meazonpro1'}).json()\n",
    "    \n",
    "    # acc_token is the token to be used in the next request\n",
    "    acc_token = 'Bearer' + ' ' + r['token']\n",
    "    \n",
    "    # get devid by serial name\n",
    "    r1 = requests.get(\n",
    "        url=address + \"/api/tenant/devices?deviceName=\" + device,\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    \n",
    "    label = r1['label']\n",
    "    devid = r1['id']['id']\n",
    "    r1 = requests.get(\n",
    "        url=address + \"/api/device/\" + devid + \"/credentials\",\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    devtoken = r1['credentialsId']\n",
    "\n",
    "    \n",
    "    return devid,acc_token,label\n",
    "\n",
    "\n",
    "def read_data(acc_token, devid, address, start_time, end_time, descriptors):\n",
    "\n",
    "    r2 = requests.get(\n",
    "        url=address + \"/api/plugins/telemetry/DEVICE/\" + devid + \"/values/timeseries?keys=\" + descriptors + \"&startTs=\" + start_time + \"&endTs=\" + end_time + \"&agg=NONE&limit=1000000\",\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    \n",
    "    if r2:\n",
    "        df = pd.DataFrame([])\n",
    "        \n",
    "        for desc in r2.keys():\n",
    "            df1 = pd.DataFrame(r2[desc])\n",
    "            df1.set_index('ts', inplace=True)\n",
    "            df1.columns = [str(desc)]\n",
    "            \n",
    "            df1.reset_index(drop=False, inplace=True)\n",
    "            df1['ts'] = pd.to_datetime(df1['ts'], unit='ms')\n",
    "            df1['ts'] = df1['ts'].dt.tz_localize('utc').dt.tz_convert('Europe/Athens')\n",
    "            df1 = df1.sort_values(by=['ts'])\n",
    "            df1.reset_index(drop=True, inplace=True)\n",
    "            df1.set_index('ts', inplace=True, drop=True)            \n",
    "            df = pd.concat([df, df1], axis=1)\n",
    "\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame([])\n",
    "        else:\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].astype('float64')\n",
    "    else:\n",
    "        df = pd.DataFrame([])\n",
    "        # print('Empty json!')\n",
    "    return df\n",
    "\n",
    "def current_unbalance_iec(df):\n",
    "    \"\"\"\n",
    "    Calculate the current unbalance percentage as per the IEC standard.\n",
    "    \"\"\"\n",
    "    # Calculate the mean current\n",
    "    avg_current = (df['curA']+df['curB']+df['curC']) / 3\n",
    "    \n",
    "    # Calculate the absolute deviations from the mean\n",
    "    d1 = abs(df['curA'] - avg_current)\n",
    "    d2 = abs(df['curB'] - avg_current)\n",
    "    d3 = abs(df['curC'] - avg_current)\n",
    "    \n",
    "    # Find the maximum deviation\n",
    "    max_deviation = max(d1, d2, d3)\n",
    "    \n",
    "    \n",
    "    # Calculate the current unbalance percentage\n",
    "    unbalance = (max_deviation / avg_current) * 100\n",
    "    \n",
    "    return unbalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, info, device):\n",
    "    \"\"\"\n",
    "    Remove occurrences of voltage drops\n",
    "    \"\"\"\n",
    "    nomcur = info.loc[info['Serial']==device,'Ampere'].values[0]\n",
    "    df = df[((df['vltA']>210) & (df['vltB']>210) & (df['vltC']>210))]\n",
    "    return nomcur,df\n",
    "\n",
    "def extract_features(orig_df, device, features_dict, nomcur, info, indnum):\n",
    "    features = {}\n",
    "    # orig_df[(orig_df.index.hour>7) & (orig_df.index.hour<19),'daynight'] = 'day'\n",
    "    orig_df['daynight'] = orig_df.index.hour.map(lambda hour: 'day' if 7 <= hour < 19 else 'night')\n",
    "    \n",
    "    for ph in ['A','B','C']:\n",
    "        # Extract features for each phase\n",
    "        # First remove loads under nominal current\n",
    "        df = orig_df.copy()\n",
    "        # df = df.loc[df['cur'+ph]>0.01*nomcur]\n",
    "        if df.empty:\n",
    "            print('Empty df')\n",
    "        \n",
    "        df['angle'+ph] = np.abs(df['angle'+ph])\n",
    "        \n",
    "        # Power features\n",
    "        df['norm_pwr'+ph] = (df['pwr'+ph] - df['pwr'+ph].min()) / (df['pwr'+ph].max() - df['pwr'+ph].min())\n",
    "        features['peak_to_mean'+ph] = df['norm_pwr'+ph].max() / df['norm_pwr'+ph].mean()\n",
    "\n",
    "        daymean = df.loc[df['daynight']=='day','pwr'+ph].mean()\n",
    "        nightmean = df.loc[df['daynight']=='night','pwr'+ph].mean()\n",
    "        features['day_night_ratio_'+ph] = daymean/(nightmean+ 1e-6)\n",
    "        features['day_night_stdratio_'+ph] = df.loc[df['daynight']=='day','pwr'+ph].std()/(df.loc[df['daynight']=='night','pwr'+ph].std() + 1e-6)\n",
    "        features['p25'+ph] = df['norm_pwr'+ph].quantile(0.25)\n",
    "        features['p75'+ph] = df['norm_pwr'+ph].quantile(0.75)\n",
    "\n",
    "        # V-I angle features\n",
    "        features['p25_angle_'+ph] = df['angle'+ph].quantile(0.25)\n",
    "        features['p50_angle_'+ph] = df['angle'+ph].quantile(0.5)\n",
    "        features['p75_angle_'+ph] = df['angle'+ph].quantile(0.75)\n",
    "        \n",
    "        features['angle_std'+ph] = df['angle'+ph].std()\n",
    "        features['cur_std'+ph] = df['cur'+ph].std()\n",
    "        \n",
    "    features['label'] = info.loc[info['Serial']==device,'Label'].values[0]\n",
    "    features['device'] = device\n",
    "    features_dict[indnum] = features\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel('HEDNO_info.xlsx', engine='openpyxl')\n",
    "info = info[info['Label']!='Unknown']\n",
    "\n",
    "info = info[info['Label']!='PV']\n",
    "devices = list(info['Serial'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.408.001992\n",
      "102.408.001975\n",
      "102.408.001978\n",
      "102.408.001979\n",
      "102.408.001991\n",
      "102.408.001967\n",
      "102.408.001968\n",
      "102.408.001969\n",
      "102.408.001970\n",
      "102.408.001971\n",
      "102.408.001972\n",
      "102.408.001988\n",
      "102.408.001946\n",
      "102.408.001945\n",
      "102.408.001949\n",
      "102.408.001943\n",
      "102.408.001944\n",
      "102.408.001948\n",
      "102.408.001947\n",
      "102.408.001989\n",
      "102.408.001957\n",
      "102.408.001953\n",
      "102.408.001954\n",
      "102.408.001955\n",
      "102.408.001956\n",
      "102.408.001952\n",
      "102.408.001990\n",
      "102.408.001959\n",
      "102.408.001960\n",
      "102.408.001961\n",
      "102.408.001962\n",
      "102.408.001964\n",
      "102.408.001965\n",
      "102.408.000226\n",
      "102.408.000196\n",
      "102.408.000180\n",
      "102.408.000181\n",
      "102.408.000182\n",
      "102.408.000178\n",
      "102.408.000171\n",
      "102.402.002103\n",
      "102.408.000221\n",
      "102.408.000217\n",
      "102.408.000209\n",
      "102.408.000202\n",
      "102.408.000207\n",
      "102.408.000227\n",
      "102.408.000199\n",
      "102.408.000222\n",
      "102.408.000204\n",
      "102.408.000225\n",
      "102.408.000205\n",
      "102.408.000793\n",
      "102.408.000224\n",
      "102.408.000195\n",
      "102.408.000228\n",
      "102.408.000194\n",
      "102.408.000192\n",
      "102.408.000211\n",
      "102.408.000216\n",
      "102.408.000229\n",
      "102.408.000764\n",
      "102.408.000197\n",
      "102.408.000183\n",
      "102.408.000172\n",
      "102.408.000173\n",
      "102.408.000189\n",
      "102.408.000185\n",
      "102.408.000184\n",
      "102.408.000186\n",
      "102.408.000212\n",
      "102.408.000215\n",
      "102.408.000210\n",
      "102.408.000200\n",
      "102.408.000203\n",
      "102.408.000206\n",
      "102.408.000213\n",
      "102.408.000214\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "address = 'https://mi6.meazon.com'\n",
    "r = requests.post(address + \"/api/auth/login\",\n",
    "                    json={'username': 'meazonpro@meazon.com', 'password': 'meazonpro1'}).json()\n",
    "\n",
    "acc_token = 'Bearer' + ' ' + r['token']\n",
    "\n",
    "\n",
    "entityId = '47545f30-5b7f-11ee-b2c9-653b42f73605' # DEDDHE ATHINAS\n",
    "r1 = requests.get(url=address + \"/api/entityGroup/\"+entityId+\"/entities?pageSize=1000&page=0\",headers={'Content-Type': 'application/json', \n",
    "'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "\n",
    "interval = 1 # interval in minutes\n",
    "descriptors = 'pwrA,pwrB,pwrC,curA,curB,curC,angleA,angleB,angleC,vltA,vltB,vltC'\n",
    "start_time = '1725829200000'# for training\n",
    "end_time = '1726434000000'\n",
    "# start_time = '1726434000000'# for testing\n",
    "# end_time = '1727038800000'\n",
    "\n",
    "features_dict = {}\n",
    "indnum = 0\n",
    "for i in range(0,len(r1['data'])):\n",
    "    assetid = r1['data'][i]['id']['id']\n",
    "    assetname = r1['data'][i]['name']\n",
    "    if assetname[0]!='0':  \n",
    "        r2 = requests.get(url=address + \"/api/relations/info?fromId=\"+assetid+\"&fromType=ASSET\",headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "        for j in range(0, len(r2)):\n",
    "            device = r2[j]['toName']\n",
    "            if device in devices:\n",
    "                \n",
    "                print(device)\n",
    "                [devid, acc_token, label] = get_dev_info(device, address)\n",
    "                df = read_data(acc_token, devid, address, start_time, end_time, descriptors)\n",
    "                if not df.empty:\n",
    "                    [nomcur, df] = preprocess(df, info, device)\n",
    "                    for date in np.unique(df.index.date):\n",
    "                        daily_group = df[df.index.date == date]\n",
    "                        features_dict = extract_features(daily_group, device, features_dict, nomcur, info, indnum)\n",
    "                        indnum += 1\n",
    "                \n",
    "                # search for nested devices\n",
    "                r3 = requests.get(url=address + \"/api/relations/info?fromId=\"+devid+\"&fromType=DEVICE\",headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "                for k in range(0, len(r3)):                    \n",
    "                    device = r3[k]['toName']\n",
    "                    if device in devices:\n",
    "                        print(device)\n",
    "                        [devid, acc_token, label] = get_dev_info(device, address)\n",
    "                        df = read_data(acc_token, devid, address, start_time, end_time, descriptors)\n",
    "                        if not df.empty:\n",
    "                            [nomcur, df] = preprocess(df, info, device)\n",
    "                            for date in np.unique(df.index.date):\n",
    "                                daily_group = df[df.index.date == date]\n",
    "                                features_dict = extract_features(daily_group, device, features_dict, nomcur, info, indnum)\n",
    "                                indnum += 1\n",
    "features_df = pd.DataFrame.from_dict(features_dict, orient='index')\n",
    "\n",
    "# Reset the index if you want to convert the primary keys to a column\n",
    "features_df.reset_index(inplace=True)\n",
    "# features_df.rename(columns={'index': 'device'}, inplace=True)\n",
    "# features_df = features_df.drop('device',axis=1)\n",
    "features_df = features_df.drop('index',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peak_to_meanA</th>\n",
       "      <th>day_night_ratio_A</th>\n",
       "      <th>day_night_stdratio_A</th>\n",
       "      <th>p25A</th>\n",
       "      <th>p75A</th>\n",
       "      <th>p25_angle_A</th>\n",
       "      <th>p50_angle_A</th>\n",
       "      <th>p75_angle_A</th>\n",
       "      <th>angle_stdA</th>\n",
       "      <th>cur_stdA</th>\n",
       "      <th>...</th>\n",
       "      <th>day_night_stdratio_C</th>\n",
       "      <th>p25C</th>\n",
       "      <th>p75C</th>\n",
       "      <th>p25_angle_C</th>\n",
       "      <th>p50_angle_C</th>\n",
       "      <th>p75_angle_C</th>\n",
       "      <th>angle_stdC</th>\n",
       "      <th>cur_stdC</th>\n",
       "      <th>label</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.326394</td>\n",
       "      <td>1.486948</td>\n",
       "      <td>1.356359</td>\n",
       "      <td>0.203981</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>7.39800</td>\n",
       "      <td>9.3400</td>\n",
       "      <td>11.21100</td>\n",
       "      <td>2.904534</td>\n",
       "      <td>32.394549</td>\n",
       "      <td>...</td>\n",
       "      <td>1.259604</td>\n",
       "      <td>0.275685</td>\n",
       "      <td>0.649947</td>\n",
       "      <td>6.1385</td>\n",
       "      <td>9.0670</td>\n",
       "      <td>11.40250</td>\n",
       "      <td>3.776074</td>\n",
       "      <td>31.826552</td>\n",
       "      <td>OK</td>\n",
       "      <td>102.408.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.809980</td>\n",
       "      <td>1.612578</td>\n",
       "      <td>1.531805</td>\n",
       "      <td>0.132492</td>\n",
       "      <td>0.550037</td>\n",
       "      <td>7.92750</td>\n",
       "      <td>9.8160</td>\n",
       "      <td>11.92800</td>\n",
       "      <td>2.898040</td>\n",
       "      <td>42.682926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.978359</td>\n",
       "      <td>0.152364</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>8.7710</td>\n",
       "      <td>10.8710</td>\n",
       "      <td>12.84850</td>\n",
       "      <td>3.326084</td>\n",
       "      <td>34.719401</td>\n",
       "      <td>OK</td>\n",
       "      <td>102.408.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.483422</td>\n",
       "      <td>1.509052</td>\n",
       "      <td>1.111146</td>\n",
       "      <td>0.166827</td>\n",
       "      <td>0.587733</td>\n",
       "      <td>7.11400</td>\n",
       "      <td>9.1540</td>\n",
       "      <td>11.09425</td>\n",
       "      <td>3.002063</td>\n",
       "      <td>33.915320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911108</td>\n",
       "      <td>0.158823</td>\n",
       "      <td>0.591923</td>\n",
       "      <td>7.8910</td>\n",
       "      <td>10.1445</td>\n",
       "      <td>13.12875</td>\n",
       "      <td>3.971063</td>\n",
       "      <td>37.287739</td>\n",
       "      <td>OK</td>\n",
       "      <td>102.408.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.719702</td>\n",
       "      <td>1.450482</td>\n",
       "      <td>1.132424</td>\n",
       "      <td>0.173006</td>\n",
       "      <td>0.529542</td>\n",
       "      <td>8.25525</td>\n",
       "      <td>10.5235</td>\n",
       "      <td>12.92050</td>\n",
       "      <td>3.034743</td>\n",
       "      <td>33.473222</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653338</td>\n",
       "      <td>0.212705</td>\n",
       "      <td>0.656431</td>\n",
       "      <td>8.9185</td>\n",
       "      <td>11.4165</td>\n",
       "      <td>14.66725</td>\n",
       "      <td>3.772623</td>\n",
       "      <td>30.713326</td>\n",
       "      <td>OK</td>\n",
       "      <td>102.408.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.843205</td>\n",
       "      <td>1.572746</td>\n",
       "      <td>1.149617</td>\n",
       "      <td>0.141906</td>\n",
       "      <td>0.540938</td>\n",
       "      <td>6.45775</td>\n",
       "      <td>8.0975</td>\n",
       "      <td>9.98725</td>\n",
       "      <td>2.874082</td>\n",
       "      <td>39.204543</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461919</td>\n",
       "      <td>0.224847</td>\n",
       "      <td>0.627035</td>\n",
       "      <td>5.9080</td>\n",
       "      <td>8.9530</td>\n",
       "      <td>12.25225</td>\n",
       "      <td>4.190081</td>\n",
       "      <td>32.450451</td>\n",
       "      <td>OK</td>\n",
       "      <td>102.408.001992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   peak_to_meanA  day_night_ratio_A  day_night_stdratio_A      p25A      p75A  \\\n",
       "0       2.326394           1.486948              1.356359  0.203981  0.623853   \n",
       "1       2.809980           1.612578              1.531805  0.132492  0.550037   \n",
       "2       2.483422           1.509052              1.111146  0.166827  0.587733   \n",
       "3       2.719702           1.450482              1.132424  0.173006  0.529542   \n",
       "4       2.843205           1.572746              1.149617  0.141906  0.540938   \n",
       "\n",
       "   p25_angle_A  p50_angle_A  p75_angle_A  angle_stdA   cur_stdA  ...  \\\n",
       "0      7.39800       9.3400     11.21100    2.904534  32.394549  ...   \n",
       "1      7.92750       9.8160     11.92800    2.898040  42.682926  ...   \n",
       "2      7.11400       9.1540     11.09425    3.002063  33.915320  ...   \n",
       "3      8.25525      10.5235     12.92050    3.034743  33.473222  ...   \n",
       "4      6.45775       8.0975      9.98725    2.874082  39.204543  ...   \n",
       "\n",
       "   day_night_stdratio_C      p25C      p75C  p25_angle_C  p50_angle_C  \\\n",
       "0              1.259604  0.275685  0.649947       6.1385       9.0670   \n",
       "1              1.978359  0.152364  0.488000       8.7710      10.8710   \n",
       "2              0.911108  0.158823  0.591923       7.8910      10.1445   \n",
       "3              1.653338  0.212705  0.656431       8.9185      11.4165   \n",
       "4              1.461919  0.224847  0.627035       5.9080       8.9530   \n",
       "\n",
       "   p75_angle_C  angle_stdC   cur_stdC  label          device  \n",
       "0     11.40250    3.776074  31.826552     OK  102.408.001992  \n",
       "1     12.84850    3.326084  34.719401     OK  102.408.001992  \n",
       "2     13.12875    3.971063  37.287739     OK  102.408.001992  \n",
       "3     14.66725    3.772623  30.713326     OK  102.408.001992  \n",
       "4     12.25225    4.190081  32.450451     OK  102.408.001992  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = features_df.drop('index',axis=1)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train OK       336\n",
      "Wrong     42\n",
      "Name: label, dtype: int64\n",
      "train OK       49\n",
      "Wrong     7\n",
      "Name: label, dtype: int64\n",
      "train OK       98\n",
      "Wrong    14\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "devices_with_label = features_df.groupby('device')['label'].apply(lambda x: x.unique()[0]).reset_index()\n",
    "\n",
    "devices_train, devices_temp = train_test_split(devices_with_label, test_size=0.30, stratify=devices_with_label['label'], random_state=42)\n",
    "\n",
    "# Second split: Split the temp set into 10% validation, 20% test\n",
    "devices_val, devices_test = train_test_split(devices_temp, test_size=2/3, stratify=devices_temp['label'], random_state=42)\n",
    "\n",
    "# Get the device IDs for each set\n",
    "train_device_ids = devices_train['device']\n",
    "val_device_ids = devices_val['device']\n",
    "test_device_ids = devices_test['device']\n",
    "\n",
    "# Filter the original dataset based on the split device IDs\n",
    "train_set = features_df[features_df['device'].isin(train_device_ids)]\n",
    "val_set = features_df[features_df['device'].isin(val_device_ids)]\n",
    "test_set = features_df[features_df['device'].isin(test_device_ids)]\n",
    "\n",
    "# Check the sizes to verify the split\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(val_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "print('train',train_set['label'].value_counts())\n",
    "print('train',val_set['label'].value_counts())\n",
    "print('train',test_set['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      1.00      1.00        98\n",
      "       Wrong       1.00      1.00      1.00        14\n",
      "\n",
      "    accuracy                           1.00       112\n",
      "   macro avg       1.00      1.00      1.00       112\n",
      "weighted avg       1.00      1.00      1.00       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_train = train_set['label']  # Target column\n",
    "\n",
    "X_test = test_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_test = test_set['label']  # Target column\n",
    "\n",
    "# # First split: 80% for training+test and 20% for validation\n",
    "# X_temp, X_val, y_temp, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# # Second split: 70% of the remaining data for training and 30% for validation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, train_size=0.7, stratify=y_temp, random_state=42)\n",
    "# # Split the data into training and testing sets\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, pos_label='OK'),\n",
    "    'recall': make_scorer(recall_score, pos_label='OK'),\n",
    "    'f1': make_scorer(f1_score, pos_label='OK')\n",
    "}\n",
    "\n",
    "\n",
    "# Grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(class_weight={'OK': 1, 'Wrong': 10}, random_state=42), param_grid, cv=skf, scoring=scoring['f1'])\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create the final model using the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    class_weight={'OK': 1, 'Wrong': 10},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the final model on the full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      0.86      0.92        49\n",
      "       Wrong       0.50      1.00      0.67         7\n",
      "\n",
      "    accuracy                           0.88        56\n",
      "   macro avg       0.75      0.93      0.79        56\n",
      "weighted avg       0.94      0.88      0.89        56\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "X_val = val_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_val = val_set['label']  # Target column\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Confusion matrix to inspect individual class performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      1.00      1.00       483\n",
      "       Wrong       0.98      1.00      0.99        63\n",
      "\n",
      "    accuracy                           1.00       546\n",
      "   macro avg       0.99      1.00      1.00       546\n",
      "weighted avg       1.00      1.00      1.00       546\n",
      "\n",
      "Confusion Matrix:\n",
      "[[482   1]\n",
      " [  0  63]]\n"
     ]
    }
   ],
   "source": [
    "X = features_df.drop(columns=['label'])  # Dropping the target column\n",
    "y = features_df['label']  # Target column\n",
    "y_pred = rf_model.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# Confusion matrix to inspect individual class performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
