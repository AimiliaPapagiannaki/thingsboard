{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import sys\n",
    "import warnings\n",
    "pd.set_option('display.max_rows', None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_info(device, address):\n",
    "    \n",
    "    r = requests.post(address + \"/api/auth/login\",\n",
    "                      json={'username': 'meazonpro@meazon.com', 'password': 'meazonpro1'}).json()\n",
    "    \n",
    "    # acc_token is the token to be used in the next request\n",
    "    acc_token = 'Bearer' + ' ' + r['token']\n",
    "    \n",
    "    # get devid by serial name\n",
    "    r1 = requests.get(\n",
    "        url=address + \"/api/tenant/devices?deviceName=\" + device,\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    \n",
    "    label = r1['label']\n",
    "    devid = r1['id']['id']\n",
    "    r1 = requests.get(\n",
    "        url=address + \"/api/device/\" + devid + \"/credentials\",\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    devtoken = r1['credentialsId']\n",
    "\n",
    "    \n",
    "    return devid,acc_token,label\n",
    "\n",
    "\n",
    "def read_data(acc_token, devid, address, start_time, end_time, descriptors):\n",
    "\n",
    "    r2 = requests.get(\n",
    "        url=address + \"/api/plugins/telemetry/DEVICE/\" + devid + \"/values/timeseries?keys=\" + descriptors + \"&startTs=\" + start_time + \"&endTs=\" + end_time + \"&agg=NONE&limit=1000000\",\n",
    "        headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "    \n",
    "    if r2:\n",
    "        df = pd.DataFrame([])\n",
    "        \n",
    "        for desc in r2.keys():\n",
    "            df1 = pd.DataFrame(r2[desc])\n",
    "            df1.set_index('ts', inplace=True)\n",
    "            df1.columns = [str(desc)]\n",
    "            \n",
    "            df1.reset_index(drop=False, inplace=True)\n",
    "            df1['ts'] = pd.to_datetime(df1['ts'], unit='ms')\n",
    "            df1['ts'] = df1['ts'].dt.tz_localize('utc').dt.tz_convert('Europe/Athens')\n",
    "            df1 = df1.sort_values(by=['ts'])\n",
    "            df1.reset_index(drop=True, inplace=True)\n",
    "            df1.set_index('ts', inplace=True, drop=True)            \n",
    "            df = pd.concat([df, df1], axis=1)\n",
    "\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame([])\n",
    "        else:\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].astype('float64')\n",
    "    else:\n",
    "        df = pd.DataFrame([])\n",
    "        # print('Empty json!')\n",
    "    return df\n",
    "\n",
    "def current_unbalance_iec(df):\n",
    "    \"\"\"\n",
    "    Calculate the current unbalance percentage as per the IEC standard.\n",
    "    \"\"\"\n",
    "    # Calculate the mean current\n",
    "    avg_current = (df['curA']+df['curB']+df['curC']) / 3\n",
    "    \n",
    "    # Calculate the absolute deviations from the mean\n",
    "    d1 = abs(df['curA'] - avg_current)\n",
    "    d2 = abs(df['curB'] - avg_current)\n",
    "    d3 = abs(df['curC'] - avg_current)\n",
    "    \n",
    "    # Find the maximum deviation\n",
    "    max_deviation = max(d1, d2, d3)\n",
    "    \n",
    "    \n",
    "    # Calculate the current unbalance percentage\n",
    "    unbalance = (max_deviation / avg_current) * 100\n",
    "    \n",
    "    return unbalance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, info, device):\n",
    "    \"\"\"\n",
    "    Remove occurrences of voltage drops\n",
    "    \"\"\"\n",
    "    nomcur = info.loc[info['Serial']==device,'Ampere'].values[0]\n",
    "    df = df[((df['vltA']>210) & (df['vltB']>210) & (df['vltC']>210))]\n",
    "    return nomcur,df\n",
    "\n",
    "def calculate_unbalance(df, phase_cols):\n",
    "    # IEC current unbalance definition: (max I - min I) / mean I * 100\n",
    "    mean_current = df[phase_cols].mean(axis=1)\n",
    "    max_current = df[phase_cols].max(axis=1)\n",
    "    min_current = df[phase_cols].min(axis=1)\n",
    "    unbalance = ((max_current - min_current) / mean_current) * 100\n",
    "    return unbalance\n",
    "\n",
    "def extract_features(orig_df, device, features_dict, nomcur, info, indnum):\n",
    "    features = {}\n",
    "    # orig_df[(orig_df.index.hour>7) & (orig_df.index.hour<19),'daynight'] = 'day'\n",
    "    orig_df['daynight'] = orig_df.index.hour.map(lambda hour: 'day' if 7 <= hour < 19 else 'night')\n",
    "    \n",
    "    for ph in ['A','B','C']:\n",
    "        # Extract features for each phase\n",
    "        # First remove loads under nominal current\n",
    "        df = orig_df.copy()\n",
    "        # df = df.loc[df['cur'+ph]>0.01*nomcur]\n",
    "        if df.empty:\n",
    "            print('Empty df')\n",
    "        \n",
    "        df['angle'+ph] = np.abs(df['angle'+ph])\n",
    "        \n",
    "        # Power features\n",
    "        df['norm_pwr'+ph] = (df['pwr'+ph] - df['pwr'+ph].min()) / (df['pwr'+ph].max() - df['pwr'+ph].min())\n",
    "        features['peak_to_mean'+ph] = df['norm_pwr'+ph].max() / df['norm_pwr'+ph].mean()\n",
    "\n",
    "        daymean = df.loc[df['daynight']=='day','pwr'+ph].mean()\n",
    "        nightmean = df.loc[df['daynight']=='night','pwr'+ph].mean()\n",
    "        features['day_night_ratio_'+ph] = daymean/(nightmean+ 1e-6)\n",
    "        features['day_night_stdratio_'+ph] = df.loc[df['daynight']=='day','pwr'+ph].std()/(df.loc[df['daynight']=='night','pwr'+ph].std() + 1e-6)\n",
    "        features['p25'+ph] = df['norm_pwr'+ph].quantile(0.25)\n",
    "        features['p75'+ph] = df['norm_pwr'+ph].quantile(0.75)\n",
    "\n",
    "        # V-I angle features\n",
    "        features['p25_angle_'+ph] = df['angle'+ph].quantile(0.25)\n",
    "        features['p50_angle_'+ph] = df['angle'+ph].quantile(0.5)\n",
    "        features['p75_angle_'+ph] = df['angle'+ph].quantile(0.75)\n",
    "        features['min_angle'+ph] = df['angle'+ph].min()\n",
    "        features['max_angle'+ph] = df['angle'+ph].max()\n",
    "        features['angle_std'+ph] = df['angle'+ph].std()\n",
    "        features['cur_std'+ph] = df['cur'+ph].std()\n",
    "        \n",
    "    day_unbalance = calculate_unbalance(df.loc[df['daynight']=='day'], ['curA', 'curB', 'curC'])\n",
    "    night_unbalance = calculate_unbalance(df.loc[df['daynight']=='night'], ['curA', 'curB', 'curC'])\n",
    "    features['day_unbalance'] = day_unbalance.mean()\n",
    "    features['night_unbalance'] = night_unbalance.mean()\n",
    "    features['label'] = info.loc[info['Serial']==device,'Label'].values[0]\n",
    "    features['device'] = device\n",
    "    features_dict[indnum] = features\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel('HEDNO_info.xlsx', engine='openpyxl')\n",
    "info = info[(info['Label']!='Unknown') & (info['Label']!='PV')]\n",
    "\n",
    "devices = list(info['Serial'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.408.001992\n",
      "102.408.001975\n",
      "102.408.001978\n",
      "102.408.001979\n",
      "102.408.001991\n",
      "102.408.001967\n",
      "102.408.001968\n",
      "102.408.001969\n",
      "102.408.001970\n",
      "102.408.001971\n",
      "102.408.001972\n",
      "102.408.001988\n",
      "102.408.001946\n",
      "102.408.001945\n",
      "102.408.001949\n",
      "102.408.001943\n",
      "102.408.001944\n",
      "102.408.001948\n",
      "102.408.001947\n",
      "102.408.001989\n",
      "102.408.001957\n",
      "102.408.001953\n",
      "102.408.001954\n",
      "102.408.001955\n",
      "102.408.001956\n",
      "102.408.001952\n",
      "102.408.001990\n",
      "102.408.001959\n",
      "102.408.001960\n",
      "102.408.001961\n",
      "102.408.001962\n",
      "102.408.001964\n",
      "102.408.001965\n",
      "102.408.000226\n",
      "102.408.000196\n",
      "102.408.000180\n",
      "102.408.000181\n",
      "102.408.000182\n",
      "102.408.000178\n",
      "102.408.000171\n",
      "102.402.002103\n",
      "102.408.000221\n",
      "102.408.000217\n",
      "102.408.000209\n",
      "102.408.000202\n",
      "102.408.000207\n",
      "102.408.000227\n",
      "102.408.000199\n",
      "102.408.000222\n",
      "102.408.000204\n",
      "102.408.000225\n",
      "102.408.000205\n",
      "102.408.000793\n",
      "102.408.000224\n",
      "102.408.000195\n",
      "102.408.000228\n",
      "102.408.000194\n",
      "102.408.000192\n",
      "102.408.000211\n",
      "102.408.000216\n",
      "102.408.000229\n",
      "102.408.000764\n",
      "102.408.000197\n",
      "102.408.000183\n",
      "102.408.000172\n",
      "102.408.000173\n",
      "102.408.000189\n",
      "102.408.000185\n",
      "102.408.000184\n",
      "102.408.000186\n",
      "102.408.000212\n",
      "102.408.000215\n",
      "102.408.000210\n",
      "102.408.000200\n",
      "102.408.000203\n",
      "102.408.000206\n",
      "102.408.000213\n",
      "102.408.000214\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "address = 'https://mi6.meazon.com'\n",
    "r = requests.post(address + \"/api/auth/login\",\n",
    "                    json={'username': 'meazonpro@meazon.com', 'password': 'meazonpro1'}).json()\n",
    "\n",
    "acc_token = 'Bearer' + ' ' + r['token']\n",
    "\n",
    "\n",
    "entityId = '47545f30-5b7f-11ee-b2c9-653b42f73605' # DEDDHE ATHINAS\n",
    "r1 = requests.get(url=address + \"/api/entityGroup/\"+entityId+\"/entities?pageSize=1000&page=0\",headers={'Content-Type': 'application/json', \n",
    "'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "\n",
    "interval = 1 # interval in minutes\n",
    "descriptors = 'pwrA,pwrB,pwrC,curA,curB,curC,angleA,angleB,angleC,vltA,vltB,vltC,cosA,cosB,cosC'\n",
    "start_time = '1725829200000'# for training\n",
    "end_time = '1726434000000'\n",
    "# start_time = '1726434000000'# for testing\n",
    "# end_time = '1727038800000'\n",
    "\n",
    "features_dict = {}\n",
    "indnum = 0\n",
    "for i in range(0,len(r1['data'])):\n",
    "    assetid = r1['data'][i]['id']['id']\n",
    "    assetname = r1['data'][i]['name']\n",
    "    if assetname[0]!='0':  \n",
    "        r2 = requests.get(url=address + \"/api/relations/info?fromId=\"+assetid+\"&fromType=ASSET\",headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "        for j in range(0, len(r2)):\n",
    "            device = r2[j]['toName']\n",
    "            if device in devices:\n",
    "                \n",
    "                print(device)\n",
    "                [devid, acc_token, label] = get_dev_info(device, address)\n",
    "                df = read_data(acc_token, devid, address, start_time, end_time, descriptors)\n",
    "                if not df.empty:\n",
    "                    [nomcur, df] = preprocess(df, info, device)\n",
    "                    for date in np.unique(df.index.date):\n",
    "                        daily_group = df[df.index.date == date]\n",
    "                        features_dict = extract_features(daily_group, device, features_dict, nomcur, info, indnum)\n",
    "                        indnum += 1\n",
    "                \n",
    "                # search for nested devices\n",
    "                r3 = requests.get(url=address + \"/api/relations/info?fromId=\"+devid+\"&fromType=DEVICE\",headers={'Content-Type': 'application/json', 'Accept': '*/*', 'X-Authorization': acc_token}).json()\n",
    "                for k in range(0, len(r3)):                    \n",
    "                    device = r3[k]['toName']\n",
    "                    if device in devices:\n",
    "                        print(device)\n",
    "                        [devid, acc_token, label] = get_dev_info(device, address)\n",
    "                        df = read_data(acc_token, devid, address, start_time, end_time, descriptors)\n",
    "                        if not df.empty:\n",
    "                            [nomcur, df] = preprocess(df, info, device)\n",
    "                            for date in np.unique(df.index.date):\n",
    "                                daily_group = df[df.index.date == date]\n",
    "                                features_dict = extract_features(daily_group, device, features_dict, nomcur, info, indnum)\n",
    "                                indnum += 1\n",
    "features_df = pd.DataFrame.from_dict(features_dict, orient='index')\n",
    "\n",
    "# Reset the index if you want to convert the primary keys to a column\n",
    "features_df.reset_index(inplace=True)\n",
    "# features_df.rename(columns={'index': 'device'}, inplace=True)\n",
    "# features_df = features_df.drop('device',axis=1)\n",
    "features_df = features_df.drop('index',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test_size=0 should be either positive and smaller than the number of samples 78 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m devices_with_label \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39munique()[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m----> 3\u001b[0m devices_train, devices_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices_with_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m00\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices_with_label\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m train_device_ids \u001b[38;5;241m=\u001b[39m devices_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m test_device_ids \u001b[38;5;241m=\u001b[39m devices_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2181\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2173\u001b[0m train_size_type \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(train_size)\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2176\u001b[0m     test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2177\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2178\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m test_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2179\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (test_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m test_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2180\u001b[0m ):\n\u001b[1;32m-> 2181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_size, n_samples)\n\u001b[0;32m   2185\u001b[0m     )\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2188\u001b[0m     train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2189\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2192\u001b[0m ):\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size, n_samples)\n\u001b[0;32m   2197\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: test_size=0 should be either positive and smaller than the number of samples 78 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "devices_with_label = features_df.groupby('device')['label'].apply(lambda x: x.unique()[0]).reset_index()\n",
    "\n",
    "devices_train, devices_test = train_test_split(devices_with_label, test_size=0.1, stratify=devices_with_label['label'], random_state=42)\n",
    "\n",
    "train_device_ids = devices_train['device']\n",
    "test_device_ids = devices_test['device']\n",
    "\n",
    "train_set = features_df[features_df['device'].isin(train_device_ids)]\n",
    "test_set = features_df[features_df['device'].isin(test_device_ids)]\n",
    "\n",
    "X_test = test_set.drop(columns=['label','device'])  # Features\n",
    "y_test = test_set['label']                 # Labels\n",
    "\n",
    "# Prepare data for stratified group K-Fold\n",
    "X_train = train_set.drop(columns=['label'])  # Features\n",
    "y_train = train_set['label']                 # Labels\n",
    "groups = train_set['device']                 # Devices\n",
    "\n",
    "# Set up StratifiedGroupKFold\n",
    "skf = StratifiedGroupKFold(n_splits=5)\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "# Initialize RandomForestClassifier with class weight\n",
    "rf = RandomForestClassifier(class_weight={'OK': 1, 'Wrong': 10}, n_estimators=50, max_depth=5, random_state=42)\n",
    "# Perform the K-fold splits\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train, groups=groups)):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    print(len(y_fold_val[y_fold_val=='Wrong']))\n",
    "    # drop device\n",
    "    X_fold_train.drop('device', axis=1, inplace=True)\n",
    "    X_fold_val.drop('device', axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    # Fit the model\n",
    "    rf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred = rf.predict(X_fold_val)\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_fold_val, y_pred)\n",
    "    f1 = f1_score(y_fold_val, y_pred, pos_label='OK')  # F1 score for the 'OK' class\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Fold {fold+1} - Accuracy: {acc}, F1 Score (OK): {f1}\")\n",
    "# Calculate and display the mean performance across all folds\n",
    "print(f\"Mean Accuracy: {sum(accuracies)/len(accuracies)}\")\n",
    "print(f\"Mean F1 Score (OK): {sum(f1_scores)/len(f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      1.00      1.00        49\n",
      "       Wrong       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        56\n",
      "   macro avg       1.00      1.00      1.00        56\n",
      "weighted avg       1.00      1.00      1.00        56\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "X_test = test_set.drop(columns=['label','device'])  # Features\n",
    "y_test = test_set['label'] \n",
    "rf.fit(X_train.drop('device',axis=1), y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix to inspect individual class performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devices_with_label = features_df.groupby('device')['label'].apply(lambda x: x.unique()[0]).reset_index()\n",
    "\n",
    "# devices_train, devices_temp = train_test_split(devices_with_label, test_size=0.30, stratify=devices_with_label['label'], random_state=42)\n",
    "\n",
    "# # Second split: Split the temp set into 10% validation, 20% test\n",
    "# devices_val, devices_test = train_test_split(devices_temp, test_size=2/3, stratify=devices_temp['label'], random_state=42)\n",
    "\n",
    "# # Get the device IDs for each set\n",
    "# train_device_ids = devices_train['device']\n",
    "# val_device_ids = devices_val['device']\n",
    "# test_device_ids = devices_test['device']\n",
    "\n",
    "# # Filter the original dataset based on the split device IDs\n",
    "# train_set = features_df[features_df['device'].isin(train_device_ids)]\n",
    "# val_set = features_df[features_df['device'].isin(val_device_ids)]\n",
    "# test_set = features_df[features_df['device'].isin(test_device_ids)]\n",
    "\n",
    "# # Check the sizes to verify the split\n",
    "# print(f\"Training set size: {len(train_set)}\")\n",
    "# print(f\"Validation set size: {len(val_set)}\")\n",
    "# print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "# print('train',train_set['label'].value_counts())\n",
    "# print('train',val_set['label'].value_counts())\n",
    "# print('train',test_set['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      1.00      1.00        98\n",
      "       Wrong       1.00      1.00      1.00        14\n",
      "\n",
      "    accuracy                           1.00       112\n",
      "   macro avg       1.00      1.00      1.00       112\n",
      "weighted avg       1.00      1.00      1.00       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = train_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_train = train_set['label']  # Target column\n",
    "\n",
    "X_test = test_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_test = test_set['label']  # Target column\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, pos_label='OK'),\n",
    "    'recall': make_scorer(recall_score, pos_label='OK'),\n",
    "    'f1': make_scorer(f1_score, pos_label='OK')\n",
    "}\n",
    "\n",
    "\n",
    "# Grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(class_weight={'OK': 1, 'Wrong': 10}, random_state=42), param_grid, cv=skf, scoring=scoring['f1'])\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create the final model using the best parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    class_weight={'OK': 1, 'Wrong': 10},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the final model on the full training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      0.86      0.92        49\n",
      "       Wrong       0.50      1.00      0.67         7\n",
      "\n",
      "    accuracy                           0.88        56\n",
      "   macro avg       0.75      0.93      0.79        56\n",
      "weighted avg       0.94      0.88      0.89        56\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 0  7]]\n"
     ]
    }
   ],
   "source": [
    "X_val = val_set.drop(columns=['label','device'])  # Dropping the target column\n",
    "y_val = val_set['label']  # Target column\n",
    "# Predict on the validation set\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Confusion matrix to inspect individual class performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          OK       1.00      1.00      1.00       483\n",
      "       Wrong       0.98      1.00      0.99        63\n",
      "\n",
      "    accuracy                           1.00       546\n",
      "   macro avg       0.99      1.00      1.00       546\n",
      "weighted avg       1.00      1.00      1.00       546\n",
      "\n",
      "Confusion Matrix:\n",
      "[[482   1]\n",
      " [  0  63]]\n"
     ]
    }
   ],
   "source": [
    "X = features_df.drop(columns=['label'])  # Dropping the target column\n",
    "y = features_df['label']  # Target column\n",
    "y_pred = rf_model.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# Confusion matrix to inspect individual class performance\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
